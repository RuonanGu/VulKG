from py2neo import Graph
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

#%% use the Bolt URL and Password of Graph DBMS to connect VulKG
# Open Neo4j Desktop and start Graph DBMS. Click Graph DBMS and the Bolt port is shown in the right sidebar.
# Only the default database of Graph DBMS can be connected.
graph = Graph("bolt://localhost:7687", auth=("neo4j", "Neo4j"))


#%% create CO_EXPLOITATION relationship
'''
creat an inferred CO_EXPLOITATION relationship based on vulnerabilities which has the same exploit. 
'''
# using WHERE id(v1) < id(v2) to prevent reduplicative relationships
query = """
CALL apoc.periodic.iterate(
  "MATCH (v1:Vulnerability)<-[:EXPLOITS]-(e:Exploit)-[:EXPLOITS]->(v2:Vulnerability)
  WHERE id(v1) < id(v2)
  RETURN v1,v2,e.eid AS eid, e.exploitPublishDate AS date",
  "MERGE (v1) <-[coexploit:CO_EXPLOITATION {coExploitEID: eid}]->(v2)
   ON CREATE SET coexploit.coExpliotPublishDate = date", 
  {batchSize: 500})
"""
res=graph.run(query).data()
print(res)

# analyse the distribution of exploit date
query = """
MATCH p=(:Vulnerability)-[r:CO_EXPLOITATION]->(:Vulnerability) 
WITH r.coExpliotPublishDate.year AS year, count(*) AS count
ORDER BY year
RETURN toString(year) AS year, count
"""
by_year = graph.run(query).to_data_frame()

plt.figure(figsize=(5,2))
plt.bar(by_year["year"].astype('int32'),by_year["count"])
plt.plot(by_year["year"].astype('int32'),by_year["count"],'-')
plt.xticks(rotation=0)
plt.xlabel('Time (year)')
plt.ylabel('No. of co-exploitations')
plt.xlim([1995,2021])
plt.grid()
plt.tight_layout()
plt.show()

# date amount
print(sum(by_year['count']))#6880
print(sum(by_year['count'][0:20])) #5349
print(sum(by_year['count'][20:]) )#1531
# It looks like 2015 would act as a good year for splitting the data.

# split positive links into training and test sets
# set coExploitationTrainNode and coExploitationTestNode property for vulnerability entities
# involved in co-exploitation based on the year 2015.

query = """
MATCH (a:Vulnerability)-[r:CO_EXPLOITATION]->(b:Vulnerability) 
where r.coExpliotPublishDate.year < 2015
SET a.coExploitationTrainNode = 1, b.coExploitationTrainNode = 1
"""
res=graph.run(query).stats()
print(res) #{'properties_set': 10698}

query = """
MATCH (a:Vulnerability)-[r:CO_EXPLOITATION]->(b:Vulnerability) 
where r.coExpliotPublishDate.year >= 2015
SET a.coExploitationTestNode = 1, b.coExploitationTestNode = 1
"""
res=graph.run(query).stats()
print(res) #{'properties_set': 3062}

# Determine how many co-exploitation relationships you have in each of these sub graphs:
# train
query = """
MATCH (a:Vulnerability)-[r:CO_EXPLOITATION]->(b:Vulnerability) 
where r.coExpliotPublishDate.year < 2015
RETURN count(*) AS count
"""
res=graph.run(query).to_data_frame()
print(res) #5349

query = """
MATCH (a:Vulnerability)
where EXISTS(a.coExploitationTrainNode)
RETURN count(a) AS count
"""
res=graph.run(query).to_data_frame()
print(res) #3320

# test
query = """
MATCH (a:Vulnerability)-[r:CO_EXPLOITATION]->(b:Vulnerability) 
where r.coExpliotPublishDate.year >= 2015
RETURN count(*) AS count
"""
res=graph.run(query).to_data_frame()
print(res) #1531

query = """
MATCH (a:Vulnerability)
where EXISTS(a.coExploitationTestNode)
RETURN count(a) AS count
"""
res=graph.run(query).to_data_frame()
print(res) #949

#%% select negative links and construct training and test sets
'''
# Positive examples
The tricky thing when working with graph data is that you cannot just randomly split the data, as this could lead to 
data leakage. 
Data leakage can occur when data outside of your training data is inadvertently used to create your model. This can 
easily happen when working with graphs because pairs of nodes in the training set may be connected to those in 
the test set.
When you compute link prediction measures over that training set the measures computed contain information from 
the test set that you will later evaluate the model against.
Instead, you need to split the graph into training and test sub graphs. If the graph has a concept of time, 
things are easier as you can split the graph at a point in time. The training set will be from before the time, 
the test set after.
This is still not a perfect solution and you must ensure that the general network structure in the training and 
test sub-graphs is similar.
Subsequently, pairs of nodes in our train and test datasets will have relationships between them. They will be 
the positive examples in your Machine Learning model.
Because the co exploit graph contains times, we can create train and test graphs by splitting the data on a 
particular year. 
The positive link examples will be taken directly from the graph.
The negative link examples will start from the positive link related nodes and end at a random vulnerability
The simplest approach for getting the negative links is to use all pair of nodes that don’t have a relationship. 
The maximum number of negative examples is equal to:
# negative examples = (# nodes)² - (# relationships) - (# nodes)
i.e. the number of nodes squared, minus the relationships that the graph has, minus self relationships.
If you were to use all of these negative examples in your training set, you would have a massive class imbalance — 
there are many negative examples and relatively few positive ones.
You need to reduce the number of negative examples. An approach described in several link prediction papers is to 
use pairs of nodes that are a specific number of hops away from each other.
This will significantly reduce the number of negative examples, although there will still be a lot more negative 
examples than positive.
To solve this problem, you either need to down sample the negative examples or up sample the positive examples.
We will take the down sampling approach. 
'''
# down_sample a dataframe
def down_sample(df):
   copy = df.copy()
   zero = Counter(copy.label.values)[0]
   un = Counter(copy.label.values)[1]
   n = zero - un  # the number that class 0 bigger than class 1
   copy = copy.drop(copy[copy.label == 0].sample(n=n, random_state=1).index)
   return copy.sample(frac=1)

# trainset construction
# train_existing_links
train_existing_links = graph.run("""
MATCH (v1:Vulnerability)<-[r:CO_EXPLOITATION]->(v2:Vulnerability) 
where r.coExpliotPublishDate.year < 2015 and id(v1) < id(v2)
RETURN id(v1) AS node1, id(v2) AS node2, 1 AS label
""").to_data_frame()  #5349

#train_missing_links
train_missing_links = pd.DataFrame(columns=['node1','node2','label'])
for id in set(train_existing_links['node1']): #1993
   params = {"vulID": id}
   # TYPE 1: random select form all vulnerabilities
   query = """
        MATCH (v1:Vulnerability)
        WHERE id(v1)<>$vulID
        WITH v1
        RETURN $vulID AS node1, id(v1) as node2, rand() AS r, 0 AS label
        ORDER BY r
        LIMIT 10
        """
   res = graph.run(query,params).to_data_frame()
   train_missing_links=pd.concat([train_missing_links, res], ignore_index=True) #19930*4

train_missing_links=train_missing_links.drop(['r'], axis=1) # drop the random factor #19930*3


# train set
training_df = pd.concat([train_missing_links, train_existing_links], ignore_index=True)
training_df['label'] = training_df['label'].astype('category')
training_df = down_sample(training_df) # downsample the majority class
training_df=training_df.reset_index(drop=True)
print(training_df.head())
print(len(training_df)) #10698
print(training_df.groupby('label').count()) #5349:5349


# testset construction
# test_existing_links
test_existing_links = graph.run("""
MATCH (v1:Vulnerability)<-[r:CO_EXPLOITATION]->(v2:Vulnerability) 
where r.coExpliotPublishDate.year >= 2015 and id(v1) < id(v2)
RETURN id(v1) AS node1, id(v2) AS node2, 1 AS label
""").to_data_frame()  #1531

#test_missing_links
test_missing_links = pd.DataFrame(columns=['node1','node2','label'])
for id in set(test_existing_links['node1']): #627
   params = {"vulID": id}
   # TYPE 1: random select form all vulnerabilities
   query = """
        MATCH (v1:Vulnerability)
        WHERE id(v1)<>$vulID
        WITH v1
        RETURN $vulID AS node1, id(v1) as node2, rand() AS r, 0 AS label
        ORDER BY r
        LIMIT 10
        """
   res = graph.run(query,params).to_data_frame()
   test_missing_links=pd.concat([test_missing_links, res], ignore_index=True)

test_missing_links=test_missing_links.drop(['r'], axis=1)


# test set
test_df = pd.concat([test_missing_links, test_existing_links], ignore_index=True)
test_df['label'] = test_df['label'].astype('category')
test_df = down_sample(test_df) # downsample the majority class
test_df=test_df.reset_index(drop=True)
print(test_df.head())
print(len(test_df)) #3062
print(test_df.groupby('label').count()) #1531:1531

#%% save co_exploitation dataset
training_df.to_pickle('./GD_VCBD_Raw/co_exploitation_training_df.pkl')
test_df.to_pickle('./GD_VCBD_Raw/co_exploitation_test_df.pkl')

